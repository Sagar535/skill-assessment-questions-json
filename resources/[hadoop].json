[{"question":" Partitioner controls the partitioning of what data?","options":[{"text":"final keys","correct":false},{"text":"final values","correct":false},{"text":"intermediate keys","correct":true},{"text":"intermediate values","correct":false}],"illustrator":""},{"question":" SQL Windowing functions are implemented in Hive using which keywords?","options":[{"text":"UNION DISTINCT, RANK","correct":false},{"text":"OVER, RANK","correct":true},{"text":"OVER, EXCEPT","correct":false},{"text":"UNION DISTINCT, RANK","correct":false}],"illustrator":""},{"question":" Rather than adding a Secondary Sort to a slow Reduce job, it is Hadoop best practice to perform which optimization?","options":[{"text":"Add a partitioned shuffle to the Map job.","correct":false},{"text":"Add a partitioned shuffle to the Reduce job.","correct":true},{"text":"Break the Reduce job into multiple, chained Reduce jobs.","correct":false},{"text":"Break the Reduce job into multiple, chained Map jobs.","correct":false}],"illustrator":""},{"question":" Hadoop Auth enforces authentication on protected resources. Once authentication has been established, it sets what type of authenticating cookie?","options":[{"text":"encrypted HTTP","correct":false},{"text":"unsigned HTTP","correct":false},{"text":"compressed HTTP","correct":false},{"text":"signed HTTP","correct":true}],"illustrator":""},{"question":" MapReduce jobs can be written in which language?","options":[{"text":"Java or Python","correct":true},{"text":"SQL only","correct":false},{"text":"SQL or Java","correct":false},{"text":"Python or SQL","correct":false}],"illustrator":""},{"question":" To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?","options":[{"text":"Reducer","correct":false},{"text":"Combiner","correct":true},{"text":"Mapper","correct":false},{"text":"Counter","correct":false}],"illustrator":""},{"question":" To verify job status, look for the value `___` in the `___`.","options":[{"text":"SUCCEEDED; syslog","correct":false},{"text":"SUCCEEDED; stdout","correct":true},{"text":"DONE; syslog","correct":false},{"text":"DONE; stdout","correct":false}],"illustrator":""},{"question":" Which line of code implements a Reducer method in MapReduce 2.0?","options":[{"text":"public void reduce(Text key, Iterator<IntWritable> values, Context context){…}","correct":true},{"text":"public static void reduce(Text key, IntWritable[] values, Context context){…}","correct":false},{"text":"public static void reduce(Text key, Iterator<IntWritable> values, Context context){…}","correct":false},{"text":"public void reduce(Text key, IntWritable[] values, Context context){…}","correct":false}],"illustrator":""},{"question":" To get the total number of mapped input records in a map job task, you should review the value of which counter?","options":[{"text":"FileInputFormatCounter","correct":false},{"text":"FileSystemCounter","correct":false},{"text":"JobCounter","correct":false},{"text":"TaskCounter (NOT SURE)","correct":true}],"illustrator":""},{"question":". Hadoop Core supports which CAP capabilities?","options":[{"text":"A, P","correct":true},{"text":"C, A","correct":false},{"text":"C, P","correct":false},{"text":"C, A, P","correct":false}],"illustrator":""},{"question":". What are the primary phases of a Reducer?","options":[{"text":"combine, map, and reduce","correct":false},{"text":"shuffle, sort, and reduce","correct":true},{"text":"reduce, sort, and combine","correct":false},{"text":"map, sort, and combine","correct":false}],"illustrator":""},{"question":". To set up Hadoop workflow with synchronization of data between jobs that process tasks both on disk and in memory, use the `___` service, which is `___`.","options":[{"text":"Oozie; open source","correct":false},{"text":"Oozie; commercial software","correct":false},{"text":"Zookeeper; commercial software","correct":false},{"text":"Zookeeper; open source","correct":true}],"illustrator":""},{"question":". For high availability, use multiple nodes of which type?","options":[{"text":"data","correct":false},{"text":"name","correct":true},{"text":"memory","correct":false},{"text":"worker","correct":false}],"illustrator":""},{"question":". DataNode supports which type of drives?","options":[{"text":"hot swappable","correct":true},{"text":"cold swappable","correct":false},{"text":"warm swappable","correct":false},{"text":"non-swappable","correct":false}],"illustrator":""},{"question":". Which method is used to implement Spark jobs?","options":[{"text":"on disk of all workers","correct":false},{"text":"on disk of the master node","correct":false},{"text":"in memory of the master node","correct":false},{"text":"in memory of all workers","correct":true}],"illustrator":""},{"question":". In a MapReduce job, where does the map() function run?","options":[{"text":"on the reducer nodes of the cluster","correct":false},{"text":"on the data nodes of the cluster (NOT SURE)","correct":true},{"text":"on the master node of the cluster","correct":false},{"text":"on every node of the cluster","correct":false}],"illustrator":""},{"question":". To reference a master file for lookups during Mapping, what type of cache should be used?","options":[{"text":"distributed cache","correct":true},{"text":"local cache","correct":false},{"text":"partitioned cache","correct":false},{"text":"cluster cache","correct":false}],"illustrator":""},{"question":". Skip bad records provides an option where a certain set of bad input records can be skipped when processing what type of data?","options":[{"text":"cache inputs","correct":false},{"text":"reducer inputs","correct":false},{"text":"intermediate values","correct":false},{"text":"map inputs","correct":true}],"illustrator":""},{"question":". Which command imports data to Hadoop from a MySQL database?","options":[{"text":"spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --warehouse-dir user/hue/oozie/deployments/spark","correct":false},{"text":"sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --warehouse-dir user/hue/oozie/deployments/sqoop","correct":false},{"text":"sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --password sqoop --warehouse-dir user/hue/oozie/deployments/sqoop","correct":true},{"text":"spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --password spark --warehouse-dir user/hue/oozie/deployments/spark","correct":false}],"illustrator":""},{"question":". In what form is Reducer output presented?","options":[{"text":"compressed (NOT SURE)","correct":true},{"text":"sorted","correct":false},{"text":"not sorted","correct":false},{"text":"encrypted","correct":false}],"illustrator":""},{"question":". Which library should be used to unit test MapReduce code?","options":[{"text":"JUnit","correct":false},{"text":"XUnit","correct":false},{"text":"MRUnit","correct":true},{"text":"HadoopUnit","correct":false}],"illustrator":""},{"question":". If you started the NameNode, then which kind of user must you be?","options":[{"text":"hadoop-user","correct":false},{"text":"super-user","correct":true},{"text":"node-user","correct":false},{"text":"admin-user","correct":false}],"illustrator":""},{"question":". State \\_\\_ between the JVMs in a MapReduce job","options":[{"text":"can be configured to be shared","correct":false},{"text":"is partially shared","correct":false},{"text":"is shared","correct":false},{"text":"is not shared (https://www.lynda.com/Hadoop-tutorials/Understanding-Java-virtual-machines-JVMs/191942/369545-4.html)","correct":true}],"illustrator":""},{"question":". To create a MapReduce job, what should be coded first?","options":[{"text":"a static job() method","correct":false},{"text":"a Job class and instance (NOT SURE)","correct":true},{"text":"a job() method","correct":false},{"text":"a static Job class","correct":false}],"illustrator":""},{"question":". To connect Hadoop to AWS S3, which client should you use?","options":[{"text":"S3A","correct":true},{"text":"S3N","correct":false},{"text":"S3","correct":false},{"text":"the EMR S3","correct":false}],"illustrator":""},{"question":". HBase works with which type of schema enforcement?","options":[{"text":"schema on write","correct":false},{"text":"no schema","correct":false},{"text":"external schema","correct":false},{"text":"schema on read","correct":false}],"illustrator":""},{"question":". HDFS file are of what type?","options":[{"text":"read-write","correct":false},{"text":"read-only","correct":false},{"text":"write-only","correct":false},{"text":"append-only","correct":false}],"illustrator":""},{"question":". A distributed cache file path can originate from what location?","options":[{"text":"hdfs or top","correct":false},{"text":"http","correct":false},{"text":"hdfs or http","correct":false},{"text":"hdfs","correct":false}],"illustrator":""},{"question":". Which library should you use to perform ETL-type MapReduce jobs?","options":[{"text":"Hive","correct":false},{"text":"Pig","correct":false},{"text":"Impala","correct":false},{"text":"Mahout","correct":false}],"illustrator":""},{"question":". What is the output of the Reducer?","options":[{"text":"a relational table","correct":false},{"text":"an update to the input file","correct":false},{"text":"a single, combined list","correct":false},{"text":"a set of <key, value> pairs","correct":true}],"illustrator":""},{"question":". To optimize a Mapper, what should you perform first?","options":[{"text":"Override the default Partitioner.","correct":false},{"text":"Skip bad records.","correct":false},{"text":"Break up Mappers that do more than one task into multiple Mappers.","correct":false},{"text":"Combine Mappers that do one task into large Mappers.","correct":false}],"illustrator":""}]